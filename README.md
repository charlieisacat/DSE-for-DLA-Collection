# Papers & Codes
This is a collection of Design Space Exploration (DSE) papers and corresponding open-source codes (if exist) for designing Deep Learning Accelerator (DLA).

## General
- (2019 ICCAD) MAGNet: A Modular Accelerator Generator for Neural Networks
- (2019 TECS) dMazeRunner: Executing Perfectly Nested Loops on Dataflow Accelerators
  - https://github.com/MPSLab-ASU/dMazeRunner
- (2019 MICRO) Understanding Reuse, Performance, and Hardware Cost of DNN Dataflows: A Data-Centric Approach
   - https://github.com/maestro-project/maestro
- (2019 ISPASS) Timeloop: A Systematic Approach to DNN Accelerator Evaluation  
  - https://github.com/NVlabs/timeloop
- (2019 ISPASS) mRNA: Enabling Efficient Mapping Space Exploration for a Reconfigurable Neural Accelerator
  - https://github.com/maeri-project/mRNA
- (2020 TACO) SMAUG: End-to-End Full-Stack Simulation Infrastructure for Deep Learning Workloads
  - https://github.com/harvard-acc/smaug
- (2020 ASPLOS) Interstellar: Using Halideâ€™s Scheduling Language to Analyze DNN Accelerators
- (2020 ICCAD) GAMMA: Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm
  - https://github.com/maestro-project/gamma
- (2021 HPCA) NeuroMeter: An Integrated Power, Area, and Timing Modeling Framework for Machine Learning Accelerators
- (2021 Trans on Comp) ZigZag: Enlarging Joint Architecture-Mapping Design Space Exploration for DNN Accelerators
  - https://github.com/ZigZag-Project/zigzag
- (2021 TECS) HW-FlowQ: A Multi-Abstraction Level HW-CNN Co-design Quantization Methodology
- (2021 IISWC) STONNE: Enabling Cycle-Level Microarchitectural Simulation for DNN Inference Accelerators
  - https://github.com/stonne-simulator/stonne
- (2021 HPCA) Heterogeneous Dataflow Accelerators for Multi-DNN Workloads
- (2021 ISCA) HASCO: Towards Agile HArdware and Software CO-design for Tensor Computation
  - https://github.com/pku-liang/HASCO
- (2022 ISPASS) Bifrost: End-to-End Evaluation and Optimization of Reconfigurable DNN Accelerators
  - https://github.com/gicLAB/bifrost
- (2022 HPCA) MAGMA: An Optimization Framework for Mapping Multiple DNNs on Multiple Accelerator Cores
  - https://github.com/maestro-project/magma
- (2022 ASPLOS) A Full-Stack Search Technique for Domain Optimized Deep Learning Accelerators
- (2022 ISCA) AMOS: Enabling Automatic Mapping for Tensor Computations On Spatial Accelerators with Hardware Abstraction
  - https://github.com/pku-liang/AMOS
## Depth-first/Layer-fusion
- (2021 IEEE Access) ConvFusion: A Model for Layer Fusion in Convolutional Neural Networks
- (2021 IEEE J Em Sel Top C) Hardware-Efficient Residual Neural Network Execution in Line-Buffer Depth-First Processing
- (2022 Arxiv) DNNFuser: Transformer as a Generalized Mapper for Fusion in DNN Accelerators
- (2022 Electronics) HW-Flow-Fusion: Inter-Layer Scheduling for Convolutional Neural Network Accelerators with Dataflow Architectures
- (2023 ASPLOS) FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks
- (2023 HPCA) DeFiNES: Enabling Fast Exploration of the Depth-first Scheduling Space for DNN Accelerators through Analytical Modeling
  - https://github.com/ZigZag-Project/DeFiNES
## Others
- (2021 IPDPS) Understanding the Design-Space of Sparse/Dense Multiphase GNN dataflows on Spatial Accelerators
  - GNN accelerator
  - https://github.com/stonne-simulator/omega
- (2022 MICRO) Sparseloop: An Analytical Approach To Sparse Tensor Accelerator Modeling
  - Sparse accelerator
  - https://github.com/Accelergy-Project/micro22-sparseloop-artifact